{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "\n",
    "from sigma_graph.data.file_manager import check_dir, find_file_in_dir, load_graph_files\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMS = 27\n",
    "OUTPUT_DIMS = 5\n",
    "NUM_EPOCHS = 500000\n",
    "TRAJECTORY_LENGTH = 10\n",
    "BATCH_SIZE = 100\n",
    "START_NODE = 21\n",
    "LEARNING_RATE = 3e-4\n",
    "GOAL_STATE = 17\n",
    "\n",
    "transitions = {\n",
    "    # 0: no-op \n",
    "    # 1: N \n",
    "    # 2: S \n",
    "    # 3: W\n",
    "    # 4: E\n",
    "    1: {0: 1, 1: 2, 2: 1, 3: 3, 4: 1},\n",
    "    2: {0: 2, 1: 12, 2: 1, 3: 2, 4: 2},\n",
    "    3: {0: 3, 1: 3, 2: 3, 3: 4, 4: 1},\n",
    "    4: {0: 4, 1: 5, 2: 4, 3: 6, 4: 3},\n",
    "    5: {0: 5, 1: 13, 2: 4, 3: 5, 4: 5},\n",
    "    6: {0: 6, 1: 6, 2: 6, 3: 7, 4: 4},\n",
    "    7: {0: 7, 1: 8, 2: 7, 3: 9, 4: 6},\n",
    "    8: {0: 8, 1: 14, 2: 7, 3: 8, 4: 8},\n",
    "    9: {0: 9, 1: 9, 2: 9, 3: 10, 4: 7},\n",
    "    10: {0: 10, 1: 11, 2: 10, 3: 10, 4: 9},\n",
    "    11: {0: 11, 1: 15, 2: 10, 3: 16, 4: 11},\n",
    "    12: {0: 12, 1: 18, 2: 2, 3: 12, 4: 12},\n",
    "    13: {0: 13, 1: 19, 2: 5, 3: 13, 4: 13},\n",
    "    14: {0: 14, 1: 20, 2: 8, 3: 14, 4: 14},\n",
    "    15: {0: 15, 1: 21, 2: 11, 3: 17, 4: 15},\n",
    "    16: {0: 16, 1: 17, 2: 16, 3: 16, 4: 11},\n",
    "    17: {0: 17, 1: 22, 2: 16, 3: 23, 4: 15},\n",
    "    18: {0: 18, 1: 18, 2: 12, 3: 25, 4: 18},\n",
    "    19: {0: 19, 1: 19, 2: 13, 3: 26, 4: 25},\n",
    "    20: {0: 20, 1: 20, 2: 14, 3: 27, 4: 26},\n",
    "    21: {0: 21, 1: 21, 2: 15, 3: 22, 4: 27},\n",
    "    22: {0: 22, 1: 22, 2: 17, 3: 24, 4: 21},\n",
    "    23: {0: 23, 1: 24, 2: 17, 3: 23, 4: 17},\n",
    "    24: {0: 24, 1: 22, 2: 23, 3: 24, 4: 22},\n",
    "    25: {0: 25, 1: 25, 2: 25, 3: 19, 4: 18},\n",
    "    26: {0: 26, 1: 26, 2: 26, 3: 20, 4: 19},\n",
    "    27: {0: 27, 1: 27, 2: 27, 3: 21, 4: 20}\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBModel(nn.Module):\n",
    "  def __init__(self, num_hid):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp_forward = nn.Sequential(nn.Linear(INPUT_DIMS, num_hid), \n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Linear(num_hid, OUTPUT_DIMS))\n",
    "    \n",
    "    self.mlp_backward = nn.Sequential(nn.Linear(INPUT_DIMS, num_hid), \n",
    "                                      nn.LeakyReLU(),\n",
    "                                      nn.Linear(num_hid, OUTPUT_DIMS))\n",
    "    \n",
    "    self.logZ = nn.Parameter(torch.ones(1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    P_F = self.mlp_forward(x)\n",
    "    return P_F\n",
    "  \n",
    "  def backward(self, x):\n",
    "    P_B = torch.tensor([(1/INPUT_DIMS)]*INPUT_DIMS)#self.mlp_backward(x) \n",
    "\n",
    "    return P_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very basic toy env \n",
    "goal_trajectory = [24, 22, 21, 27, 20, 14, 8, 7, 6, 4, 3, 1]\n",
    "\n",
    "def compute_reward(trajectory):\n",
    "    # reward = 0\n",
    "    # for i in range(len(trajectory)):\n",
    "    #     if trajectory[i] == goal_trajectory[i]:\n",
    "    #         reward = reward + 1\n",
    "    # return torch.tensor(reward)\n",
    "    if trajectory[-1] == 17:\n",
    "       return 1\n",
    "    return 0\n",
    "\n",
    "model = TBModel(512)\n",
    "opt = torch.optim.Adam(model.parameters(),  3e-4)\n",
    "\n",
    "tb_losses = []\n",
    "tb_rewards = []\n",
    "logZs = []\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "\n",
    "for episode in tqdm.tqdm(range(NUM_EPOCHS), ncols=40):\n",
    "  state = START_NODE\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "\n",
    "  trajectory = []\n",
    "  for t in range(TRAJECTORY_LENGTH):\n",
    "    trajectory.append(state)\n",
    "    P_F_s = model.forward(state_to_vec(state))\n",
    "    P_B_s = model.backward(state_to_vec(state))\n",
    "\n",
    "    cat = Categorical(logits=P_F_s)\n",
    "    action = cat.sample()\n",
    "    new_state = transitions[state][action.item()]\n",
    "    total_P_F += cat.log_prob(action)\n",
    "\n",
    "    total_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "  reward = compute_reward(trajectory)\n",
    "  loss = (model.logZ + total_P_F - torch.log(reward).clip(-20) - total_P_B).pow(2)\n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += reward\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    tb_losses.append(minibatch_loss.item())\n",
    "    tb_rewards.append(minibatch_reward.item()/BATCH_SIZE)\n",
    "    minibatch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    logZs.append(model.logZ.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = pp.subplots(3, 1, figsize=(10,6))\n",
    "pp.sca(ax[0])\n",
    "pp.plot(tb_losses)\n",
    "pp.yscale('log')\n",
    "pp.ylabel('loss')\n",
    "pp.sca(ax[1])\n",
    "pp.plot(np.exp(logZs))\n",
    "pp.ylabel('estimated Z')\n",
    "pp.sca(ax[2])\n",
    "pp.plot(tb_rewards)\n",
    "pp.ylabel('rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = START_NODE\n",
    "\n",
    "for t in range(TRAJECTORY_LENGTH):\n",
    "    print(f'state {state}')\n",
    "    P_F_s = model.forward(state_to_vec(state))\n",
    "    P_B_s = model.backward(state_to_vec(state))\n",
    "\n",
    "    cat = Categorical(logits=P_F_s)\n",
    "    action = cat.sample()\n",
    "    new_state = transitions[state][action.item()]\n",
    "    total_P_F += cat.log_prob(action)\n",
    "\n",
    "    total_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
    "\n",
    "    state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = 0\n",
    "for i in range(1000):\n",
    "    state = START_NODE\n",
    "    for t in range(TRAJECTORY_LENGTH):\n",
    "        P_F_s = model.forward(state_to_vec(state))\n",
    "        P_B_s = model.backward(state_to_vec(state))\n",
    "\n",
    "        cat = Categorical(logits=P_F_s)\n",
    "        action = cat.sample()\n",
    "        new_state = transitions[state][action.item()]\n",
    "        total_P_F += cat.log_prob(action)\n",
    "\n",
    "        total_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    if state == 17:\n",
    "        hit = hit + 1\n",
    "print(hit/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dirs = {}\n",
    "for node in range(27):\n",
    "    probs = model.forward(state_to_vec(node))\n",
    "    # 0: no-op \n",
    "    # 1: N \n",
    "    # 2: S \n",
    "    # 3: W\n",
    "    # 4: E\n",
    "    total_probs = {}\n",
    "    total_probs[\"NOOP\"] = (probs[0]).tolist()\n",
    "    total_probs[\"N\"] = (probs[1]).tolist()\n",
    "    total_probs[\"S\"] = (probs[2]).tolist()\n",
    "    total_probs[\"W\"] = (probs[3]).tolist()\n",
    "    total_probs[\"E\"] = (probs[4]).tolist()\n",
    " \n",
    "\n",
    "    state_dirs[node] = total_probs\n",
    "\n",
    "flows = state_dirs\n",
    "print(f'flows {flows[23]}')\n",
    "print(f'flows {flows[18]}')\n",
    "print(f'flows {flows[15]}')\n",
    "map_info, _ = load_graph_files(map_lookup=\"S\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_alpha(0.)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "col_map = [\"gold\"] * len(map_info.n_info)\n",
    "\n",
    "nx.draw_networkx(map_info.g_acs, map_info.n_info, node_color=col_map, edge_color=\"blue\", arrows=True)\n",
    "\n",
    "max_x = max(pos[0] for pos in map_info.n_info.values())\n",
    "max_y = max(pos[1] for pos in map_info.n_info.values())\n",
    "scale_factor = max(max_x, max_y) / 2000\n",
    "op_to_dir = {\n",
    "    'N': (0, 1), \n",
    "    'S': (0, -1), \n",
    "    'W': (-1, 0),\n",
    "    'E': (1, 0)\n",
    "}\n",
    "for node in map_info.g_acs.nodes:\n",
    "    pos = map_info.n_info[node]\n",
    "    x, y = pos\n",
    "    dir_flows = flows[node-1]\n",
    "\n",
    "    color_vals = np.linspace(0, 1.00, 100)\n",
    "    colors = [(color, 0, 0, 0) for color in color_vals]\n",
    "\n",
    "    for op in ['N', 'S', 'W', 'E']:\n",
    "        flow = int((dir_flows[op]) * 99)\n",
    "        dx, dy = op_to_dir[op]\n",
    "        plt.arrow(x, y, dx*scale_factor, dy*scale_factor, color=colors[flow], alpha=1, width=0.1, head_width=1)\n",
    "\n",
    "    circle_radius = 1.00 \n",
    "    circle_center_x = x + circle_radius  \n",
    "    circle_center_y = y + circle_radius \n",
    "\n",
    "    flow = int((dir_flows['NOOP']) * 99)\n",
    "\n",
    "    circle = plt.Circle((circle_center_x, circle_center_y), circle_radius, color=colors[flow], alpha=0.8, fill=False, linewidth=2)\n",
    "\n",
    "    plt.gca().add_patch(circle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows \n",
    "{0: {'NOOP': -0.04281928390264511, 'N': -0.7044185400009155, 'S': 0.8365095853805542, 'W': -0.294208288192749, 'E': -0.0060260482132434845}, \n",
    " 1: {'NOOP': -0.06572075188159943, 'N': -0.8022883534431458, 'S': 0.7774446606636047, 'W': -0.23733994364738464, 'E': -0.21175378561019897}, \n",
    " 2: {'NOOP': -0.15315857529640198, 'N': -0.8222216963768005, 'S': 0.9140884280204773, 'W': -0.20453889667987823, 'E': -0.2380082607269287}, \n",
    " 3: {'NOOP': -0.15023279190063477, 'N': -0.7378121614456177, 'S': 0.8891063332557678, 'W': -0.09142963588237762, 'E': -0.24385100603103638}, \n",
    " 4: {'NOOP': -0.07206474989652634, 'N': -1.0085142850875854, 'S': 0.9080254435539246, 'W': 0.5623538494110107, 'E': -0.8295060396194458}, \n",
    " 5: {'NOOP': -0.17401105165481567, 'N': -1.0206310749053955, 'S': 1.168321132659912, 'W': -0.31326058506965637, 'E': -0.29284152388572693}, \n",
    " 6: {'NOOP': 0.29616788029670715, 'N': -0.6765700578689575, 'S': 1.7027835845947266, 'W': -0.7930687665939331, 'E': -0.9280052185058594}, \n",
    " 7: {'NOOP': -0.18688106536865234, 'N': -1.0172914266586304, 'S': 0.8644939064979553, 'W': -0.6106293201446533, 'E': 0.5325061082839966}, \n",
    " 8: {'NOOP': -0.04091595858335495, 'N': -0.8219321370124817, 'S': 1.088913083076477, 'W': -0.28339526057243347, 'E': -0.32663142681121826}, \n",
    " 9: {'NOOP': -0.08009160310029984, 'N': -0.7991769313812256, 'S': 0.7578392624855042, 'W': -0.4435887336730957, 'E': -0.05031454563140869}, \n",
    " 10: {'NOOP': -0.03729896992444992, 'N': -0.8757666349411011, 'S': 1.041592001914978, 'W': -0.47257521748542786, 'E': -0.15408331155776978}, 11: {'NOOP': -0.06113097816705704, 'N': -0.7633384466171265, 'S': 1.0141456127166748, 'W': -0.3527575731277466, 'E': -0.3120118975639343}, 12: {'NOOP': -0.00613616406917572, 'N': -0.7722690105438232, 'S': 0.938063383102417, 'W': -0.2874240577220917, 'E': -0.3028886318206787}, 13: {'NOOP': -0.1158658042550087, 'N': -0.9718967080116272, 'S': 1.1266144514083862, 'W': -0.3259008824825287, 'E': -0.30215832591056824}, 14: {'NOOP': -0.15581300854682922, 'N': -0.9797416925430298, 'S': 1.1761813163757324, 'W': -0.27505865693092346, 'E': -0.4173281788825989}, 15: {'NOOP': 0.04006331041455269, 'N': -0.7302824258804321, 'S': 0.9193851947784424, 'W': -0.28476497530937195, 'E': -0.3234466314315796}, 16: {'NOOP': -0.028649136424064636, 'N': -0.7955811619758606, 'S': 0.8990495800971985, 'W': -0.18944144248962402, 'E': -0.26828718185424805}, 17: {'NOOP': -0.03757932037115097, 'N': -0.7265559434890747, 'S': 0.9418012499809265, 'W': -0.3114413321018219, 'E': -0.2813953459262848}, 18: {'NOOP': 0.07390851527452469, 'N': -0.7424029111862183, 'S': 1.0141987800598145, 'W': -0.29449424147605896, 'E': -0.34659188985824585}, 19: {'NOOP': -0.1182183101773262, 'N': -0.9873968362808228, 'S': 1.3456995487213135, 'W': -0.40572383999824524, 'E': -0.5384065508842468}, 20: {'NOOP': 0.011343080550432205, 'N': -0.7675076127052307, 'S': 1.1061038970947266, 'W': -0.49046817421913147, 'E': -0.3443695902824402}, 21: {'NOOP': -0.09412992745637894, 'N': -0.8214873671531677, 'S': 0.954239547252655, 'W': -0.25977054238319397, 'E': -0.19500136375427246}, 22: {'NOOP': -0.034168392419815063, 'N': -0.8640755414962769, 'S': 0.9225086569786072, 'W': -0.20460711419582367, 'E': -0.21131184697151184}, 23: {'NOOP': -0.06955473870038986, 'N': -0.7043285369873047, 'S': 0.9922298789024353, 'W': -0.3274853527545929, 'E': -0.18804946541786194}, 24: {'NOOP': -0.09201907366514206, 'N': -0.7363082766532898, 'S': 0.9292238354682922, 'W': -0.2653632164001465, 'E': -0.21761581301689148}, 25: {'NOOP': -0.08425996452569962, 'N': -0.745369553565979, 'S': 0.8916536569595337, 'W': -0.31660374999046326, 'E': -0.2894901633262634}, 26: {'NOOP': -0.12031040340662003, 'N': -0.7619705200195312, 'S': 0.6998625993728638, 'W': -0.23402869701385498, 'E': -0.0803600326180458}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uk3p11j5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d035315df74d4809b2440ec74187a940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Z</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>loss</td><td>█▆▄▅▄▃▃▃▃▃▃▂▃▂▃▂▃▂▃▃▂▂▂▂▃▃▂▂▂▁▂▂▂▂▂▁▁▂▂▂</td></tr><tr><td>reward</td><td>▁▃▃▃▂▄▄▄▄▆▅▅▅▇▄▄▅▅▆▆▅▅▅▇▃▄▆▅▅█▅▃▅▅▅▆▆▅▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Z</td><td>0.91824</td></tr><tr><td>loss</td><td>63.36554</td></tr><tr><td>reward</td><td>1.38</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-darkness-559</strong> at: <a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/uk3p11j5' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation/runs/uk3p11j5</a><br/> View project at: <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240519_212745-uk3p11j5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uk3p11j5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a89a7e050f4aad9dd5b9fecffa7ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112684678907195, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/wandb/run-20240519_212912-yuwulg5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/yuwulg5i' target=\"_blank\">expert-morning-560</a></strong> to <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/yuwulg5i' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation/runs/yuwulg5i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/r-marr747/graph-training-simulation/runs/yuwulg5i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f78780acb50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"graph-training-simulation\",\n",
    "    config={\n",
    "            \"exp_config\": {\n",
    "                \"learning_rate\": LEARNING_RATE,\n",
    "                \"epocs\": NUM_EPOCHS,\n",
    "                \"batch_size\": BATCH_SIZE\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|▋| 359401/500000 [09:26<03:41, 634.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m P_F_s \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(gflow_state)\n\u001b[1;32m     95\u001b[0m P_B_s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m)]) \u001b[38;5;66;03m#model.backward(gflow_state)\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m cat \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP_F_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m action \u001b[38;5;241m=\u001b[39m cat\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     99\u001b[0m _gflow_state \u001b[38;5;241m=\u001b[39m gflow_state\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/.venv/lib/python3.8/site-packages/torch/distributions/categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Transitions on graph\n",
    "transitions = {\n",
    "    # 0: no-op \n",
    "    # 1: N \n",
    "    # 2: S \n",
    "    # 3: W\n",
    "    # 4: E\n",
    "    1: {0: 1, 1: 2, 2: 1, 3: 3, 4: 1},\n",
    "    2: {0: 2, 1: 12, 2: 1, 3: 2, 4: 2},\n",
    "    3: {0: 3, 1: 3, 2: 3, 3: 4, 4: 1},\n",
    "    4: {0: 4, 1: 5, 2: 4, 3: 6, 4: 3},\n",
    "    5: {0: 5, 1: 13, 2: 4, 3: 5, 4: 5},\n",
    "    6: {0: 6, 1: 6, 2: 6, 3: 7, 4: 4},\n",
    "    7: {0: 7, 1: 8, 2: 7, 3: 9, 4: 6},\n",
    "    8: {0: 8, 1: 14, 2: 7, 3: 8, 4: 8},\n",
    "    9: {0: 9, 1: 9, 2: 9, 3: 10, 4: 7},\n",
    "    10: {0: 10, 1: 11, 2: 10, 3: 10, 4: 9},\n",
    "    11: {0: 11, 1: 15, 2: 10, 3: 16, 4: 11},\n",
    "    12: {0: 12, 1: 18, 2: 2, 3: 12, 4: 12},\n",
    "    13: {0: 13, 1: 19, 2: 5, 3: 13, 4: 13},\n",
    "    14: {0: 14, 1: 20, 2: 8, 3: 14, 4: 14},\n",
    "    15: {0: 15, 1: 21, 2: 11, 3: 17, 4: 15},\n",
    "    16: {0: 16, 1: 17, 2: 16, 3: 16, 4: 11},\n",
    "    17: {0: 17, 1: 22, 2: 16, 3: 23, 4: 15},\n",
    "    18: {0: 18, 1: 18, 2: 12, 3: 25, 4: 18},\n",
    "    19: {0: 19, 1: 19, 2: 13, 3: 26, 4: 25},\n",
    "    20: {0: 20, 1: 20, 2: 14, 3: 27, 4: 26},\n",
    "    21: {0: 21, 1: 21, 2: 15, 3: 22, 4: 27},\n",
    "    22: {0: 22, 1: 22, 2: 17, 3: 24, 4: 21},\n",
    "    23: {0: 23, 1: 24, 2: 17, 3: 23, 4: 17},\n",
    "    24: {0: 24, 1: 22, 2: 23, 3: 24, 4: 22},\n",
    "    25: {0: 25, 1: 25, 2: 25, 3: 19, 4: 18},\n",
    "    26: {0: 26, 1: 26, 2: 26, 3: 20, 4: 19},\n",
    "    27: {0: 27, 1: 27, 2: 27, 3: 21, 4: 20}\n",
    "}\n",
    "\n",
    "# Model\n",
    "class TBModel(nn.Module):\n",
    "  def __init__(self, num_hid):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp_forward = nn.Sequential(nn.Linear(TRAJECTORY_LENGTH, num_hid), \n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Linear(num_hid, OUTPUT_DIMS))\n",
    "    \n",
    "    self.mlp_backward = nn.Sequential(nn.Linear(TRAJECTORY_LENGTH, num_hid), \n",
    "                                      nn.LeakyReLU(),\n",
    "                                      nn.Linear(num_hid, OUTPUT_DIMS))\n",
    "    \n",
    "    self.logZ = nn.Parameter(torch.ones(1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    P_F = self.mlp_forward(x)\n",
    "    return P_F\n",
    "  \n",
    "  def backward(self, x):\n",
    "    P_B = torch.tensor([(1/INPUT_DIMS)]*INPUT_DIMS)#self.mlp_backward(x) \n",
    "\n",
    "    return P_B\n",
    "\n",
    "\n",
    "def reward_that_model_learns(trajectory):\n",
    "  if trajectory[-1] == 20:\n",
    "    return torch.tensor([1])\n",
    "  return torch.tensor([0])\n",
    "\n",
    "def partial_trajectory(trajectory):\n",
    "  goal_trajectory = [21, 27, 20, 14, 8, 7, 9, 10, 11, 16]\n",
    "  reward = 0\n",
    "  for i in range(len(trajectory)):\n",
    "    if trajectory[i] == goal_trajectory[i]:\n",
    "      reward = reward + 1\n",
    "  return torch.tensor(reward)\n",
    "\n",
    "model = TBModel(512)\n",
    "opt = torch.optim.Adam(model.parameters(),  3e-4)\n",
    "\n",
    "tb_losses = []\n",
    "tb_rewards = []\n",
    "logZs = []\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "\n",
    "for episode in tqdm.tqdm(range(NUM_EPOCHS), ncols=40):\n",
    "  \n",
    "  gflow_state = torch.zeros(TRAJECTORY_LENGTH)\n",
    "  state = START_NODE\n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  trajectory = []\n",
    "\n",
    "  for t in range(TRAJECTORY_LENGTH):\n",
    "    trajectory.append(state)\n",
    "    P_F_s = model.forward(gflow_state)\n",
    "    P_B_s = torch.tensor([(1/5)]) #model.backward(gflow_state)\n",
    "\n",
    "    cat = Categorical(logits=P_F_s)\n",
    "    action = cat.sample()\n",
    "    _gflow_state = gflow_state.clone()\n",
    "    _gflow_state[t] = action\n",
    "    gflow_state = _gflow_state.clone()\n",
    "\n",
    "    new_state = transitions[state][action.item()]\n",
    "    total_P_F += cat.log_prob(action)\n",
    "    total_P_B += torch.log(P_B_s) #Categorical(logits=P_B_s).log_prob(action)\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "  reward = partial_trajectory(trajectory)\n",
    "\n",
    "  loss = (model.logZ + total_P_F - torch.log(reward).clip(-20) - total_P_B).pow(2)\n",
    "  \n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += reward\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    wandb.log({\n",
    "      \"loss\": minibatch_loss.item(),\n",
    "      \"reward\": minibatch_reward.item()/BATCH_SIZE,\n",
    "      \"Z\": model.logZ.item()\n",
    "    })\n",
    "    minibatch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 5\n",
      "[21, 27, 20, 14, 8]\n"
     ]
    }
   ],
   "source": [
    "#deterministic trajectory sample\n",
    "\n",
    "gflow_state = torch.zeros(TRAJECTORY_LENGTH)\n",
    "state = START_NODE\n",
    "total_P_F = 0\n",
    "total_P_B = 0\n",
    "trajectory = []\n",
    "\n",
    "for t in range(TRAJECTORY_LENGTH):\n",
    "    trajectory.append(state)\n",
    "    P_F_s = model.forward(gflow_state)\n",
    "    \n",
    "    max_val, action = torch.max(P_F_s, dim=0)\n",
    "\n",
    "    _gflow_state = gflow_state.clone()\n",
    "    _gflow_state[t] = action\n",
    "    gflow_state = _gflow_state.clone()\n",
    "\n",
    "    new_state = transitions[state][action.item()]\n",
    "\n",
    "    state = new_state\n",
    "\n",
    "reward = partial_trajectory(trajectory)\n",
    "print(f'reward {reward}')\n",
    "\n",
    "print(trajectory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cogn_arch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
