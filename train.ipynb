{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 16:07:52,584\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.agents.dqn` has been deprecated. Use `ray.rllib.algorithms.[dqn|simple_q|r2d2|apex_dqn]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "INPUT_DIMS = 27\n",
    "OUTPUT_DIMS = 5\n",
    "NUM_EPOCHS = 100000\n",
    "TRAJECTORY_LENGTH = 20\n",
    "BATCH_SIZE = 100\n",
    "START_NODE = 25\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 10:\n",
    "        return 2\n",
    "    if state == 2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "path_data ./GflowsForSimulation/sigma_graph/data/parsed/\n",
      "/Users/ryanmarr/Documents/CognArch/GflowsForSimulation\n"
     ]
    }
   ],
   "source": [
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epocs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE\n",
    "        }\n",
    "    )\n",
    "\n",
    "config = {\n",
    "    \"custom_model\": \"gnn_custom\",\n",
    "    \"reward\": \"toy\",\n",
    "    \"custom_model_config\": {\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "LEARNING_RATE = 3e-4\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "# sampler_fcn -> sampler\n",
    "# TODO: this is bad revisit and come up with something more concise\n",
    "\n",
    "#optimizer = optim.AdamW(gflowfigure8.sampler.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|        | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiGraph with 27 nodes and 64 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Either `probs` or `logits` must be specified, but not both.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(TRAJECTORY_LENGTH):\n\u001b[0;32m---> 19\u001b[0m   step \u001b[39m=\u001b[39m gflowfigure8\u001b[39m.\u001b[39;49mstep(TEMP_AGENT_INDEX)  \n\u001b[1;32m     20\u001b[0m   total_P_F \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m step[\u001b[39m'\u001b[39m\u001b[39mforward_prob\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m   total_P_B \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m step[\u001b[39m'\u001b[39m\u001b[39mbackward_prob\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/CognArch/GflowsForSimulation/sigma_graph/envs/figure8/gflow_figure8_squad.py:171\u001b[0m, in \u001b[0;36mGlowFigure8Squad.step\u001b[0;34m(self, a_id)\u001b[0m\n\u001b[1;32m    168\u001b[0m probs_forward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mforward(torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates[a_id],], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint8), device\u001b[39m=\u001b[39mdevice))\n\u001b[1;32m    170\u001b[0m \u001b[39m# TODO fix thiss\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m cat \u001b[39m=\u001b[39m Categorical(logits\u001b[39m=\u001b[39;49mprobs_forward)\n\u001b[1;32m    172\u001b[0m discrete_action \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39msample()\n\u001b[1;32m    173\u001b[0m forward_prob \u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mlog_prob(discrete_action)\n",
      "File \u001b[0;32m~/Documents/CognArch/.cogn_arch/lib/python3.10/site-packages/torch/distributions/categorical.py:53\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, probs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, logits\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, validate_args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     52\u001b[0m     \u001b[39mif\u001b[39;00m (probs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m (logits \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     54\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mEither `probs` or `logits` must be specified, but not both.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m probs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m probs\u001b[39m.\u001b[39mdim() \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Either `probs` or `logits` must be specified, but not both."
     ]
    }
   ],
   "source": [
    "# Using sigma code fully\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "for episode in tqdm.tqdm(range(NUM_EPOCHS), ncols=40):\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  gflowfigure8._reset_agents()\n",
    "\n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "  for t in range(TRAJECTORY_LENGTH):\n",
    "\n",
    "    step = gflowfigure8.step(TEMP_AGENT_INDEX)  \n",
    "    total_P_F += step['forward_prob']\n",
    "    total_P_B += step['backward_prob']\n",
    "    total_reward += torch.tensor(gflowfigure8._step_reward_test())\n",
    "\n",
    "  logZ = gflowfigure8.sampler.logZ\n",
    " \n",
    "  clipped_reward = torch.log(total_reward).clip(-20)\n",
    "  loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "  \n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += clipped_reward\n",
    "  minibatch_z += logZ\n",
    "  minibatch_pf += total_P_F\n",
    "  minibatch_pb += total_P_B\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    if WANDB:\n",
    "      wandb.log({\n",
    "          \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "          \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "          \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "          \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "          \"Z\": minibatch_z/BATCH_SIZE\n",
    "        })\n",
    "      # for name, param in sampler.named_parameters():\n",
    "      #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "          \n",
    "    minibatch_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    minibatch_z = 0\n",
    "    minibatch_pf = 0\n",
    "    minibatch_pb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: there has to be a better way to do this\n",
    "def norm_dict(unnorm):\n",
    "    min_val = 9999999\n",
    "    for key in unnorm:\n",
    "        min_val = min(unnorm[key], min_val)\n",
    "    for key in unnorm:\n",
    "        unnorm[key] = unnorm[key] + (min(min_val, 0) * -1)\n",
    "    sum = 0\n",
    "    for key in unnorm:\n",
    "        sum += unnorm[key]\n",
    "    for key in unnorm:\n",
    "        unnorm[key] /= sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gflowfigure8._reset_agents()\n",
    "state_dirs = {}\n",
    "for node in range(27):\n",
    "    states = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "    states[0][node] = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state = torch.tensor(np.array([states[0],], dtype=np.int8)).to(device)\n",
    "    probs = gflowfigure8.sampler.forward(state)\n",
    "    # (forward_prob, action) = gflowfigure8.probs_to_action(probs)\n",
    "    total_probs = {}\n",
    "    total_probs[\"NOOP\"] = (probs[0]+probs[5]+probs[10]).tolist()\n",
    "    total_probs[\"N\"] = (probs[1]+probs[6]+probs[11]).tolist()\n",
    "    total_probs[\"S\"] = (probs[2]+probs[7]+probs[12]).tolist()\n",
    "    total_probs[\"W\"] = (probs[3]+probs[8]+probs[13]).tolist()\n",
    "    total_probs[\"E\"] = (probs[4]+probs[9]+probs[14]).tolist()\n",
    "    \n",
    "    norm_dict(total_probs)\n",
    "\n",
    "    state_dirs[node] = total_probs\n",
    "\n",
    "json_data = json.dumps(state_dirs, indent=2)\n",
    "with open('logs/temp/flows.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "19\n",
      "13\n",
      "13\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "trajectory = Trajectory()\n",
    "gflowfigure8._reset_agents()\n",
    "for _ in range(20):   \n",
    "    for a_id in range(config['custom_model_config']['nred']):\n",
    "        step = gflowfigure8.step(a_id)\n",
    "        trajectory.add_step(\n",
    "            forward_prob=step['forward_prob'],\n",
    "            backward_prob=step['backward_prob'],\n",
    "            # flow=step['flow'],\n",
    "            # action=step['action'],\n",
    "            reward=step['step_reward'],\n",
    "            # node=step['node']\n",
    "        )\n",
    "        print(step['node'])\n",
    "        #print(step['action'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cogn_arch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
