{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 15:42:11,865\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.agents.dqn` has been deprecated. Use `ray.rllib.algorithms.[dqn|simple_q|r2d2|apex_dqn]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "from sigma_graph.data.file_manager import check_dir, find_file_in_dir, load_graph_files\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_EPOCHS = 100000\n",
    "# default = 34\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 10:\n",
    "        return 2\n",
    "    if state == 2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn-complex-coordinate-2024-04-30 15:42:12\n",
      "---------------\n",
      "path_data ./GflowsForSimulation/sigma_graph/data/parsed/\n",
      "/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation\n",
      "data_pat /home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/data/parsed/info_list_pat_0_27.pickle\n"
     ]
    }
   ],
   "source": [
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "config = {\n",
    "    \"custom_model_config\": {\n",
    "        \"custom_model\": \"fcn\", #fcn #attn_fcn\n",
    "        \"reward\": \"complex\", #random_region random single complex\n",
    "        \"reward_interval\": \"step\", #trajectory \n",
    "        \"trajectory_per_reward\": 1,\n",
    "        \"embedding\": \"coordinate\", #number #coordinate\n",
    "        \"is_dynamic_embedding\": False,\n",
    "        \"trajectory_length\": 34,\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"start_node\": 22,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "current_time = datetime.now()\n",
    "run_name = f\"{config['custom_model_config']['custom_model']}-{config['custom_model_config']['reward']}-{config['custom_model_config']['embedding']}-{current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "print(run_name)\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "                \"model_config\": config,\n",
    "                \"exp_config\": {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epocs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE\n",
    "            }\n",
    "        },\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 117/15000 [00:23<40:19,  6.15it/s]  Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f345c7d7550>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/.venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "  1%|‚ñè         | 194/15000 [00:36<39:59,  6.17it/s]"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "pbar = tqdm(total=NUM_EPOCHS)\n",
    "episode = 0\n",
    "\n",
    "while episode <= NUM_EPOCHS:\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  gflowfigure8._reset_agents()\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "\n",
    "  region_nodes = {15, 11, 10}\n",
    "\n",
    "  if config['custom_model_config']['reward'] == 'random':\n",
    "    reward_node = [random.randint(1, 27)]\n",
    "  elif config['custom_model_config']['reward'] == 'random_region':\n",
    "    reward_node = random.sample(region_nodes, 1)\n",
    "  elif config['custom_model_config']['reward'] == 'single':\n",
    "    reward_node = [17]\n",
    "  elif config['custom_model_config']['reward'] == 'complex':\n",
    "    reward_node = []\n",
    "  \n",
    "  if config['custom_model_config']['is_dynamic_embedding']:  \n",
    "    gflowfigure8.update_reward(reward_node)\n",
    "    \n",
    "  gflowfigure8.reset()\n",
    "\n",
    "  no_trajectory = config['custom_model_config']['trajectory_per_reward']\n",
    "  trajectory_length = config['custom_model_config']['trajectory_length']\n",
    "\n",
    "  for trajectory in range(no_trajectory):\n",
    "    trajectory_path = []\n",
    "    action_path = []\n",
    "    for t in range(trajectory_length):\n",
    "      step = gflowfigure8.step(TEMP_AGENT_INDEX, reward_node)  \n",
    "      total_P_F += step['forward_prob']\n",
    "      total_P_B += step['backward_prob']\n",
    "      total_reward += step['step_reward']\n",
    "      trajectory_path.append(step['node'])\n",
    "      action_path.append(step['action'])\n",
    "\n",
    "    logZ = gflowfigure8.sampler.logZ\n",
    "    # TODO find more elegant solution to nan issue\n",
    "    if config['custom_model_config']['reward'] == 'complex':\n",
    "      total_reward += gflowfigure8._episode_rewards()[0]\n",
    "      # maybe +100 is not good\n",
    "      clipped_reward = torch.log(torch.tensor(total_reward+100)).clip(-20)\n",
    "\n",
    "    # clipped_reward = torch.log(torch.tensor(end_rewards)).clip(-20)\n",
    "\n",
    "    loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "\n",
    "    minibatch_loss += loss\n",
    "    minibatch_reward += clipped_reward\n",
    "    minibatch_z += logZ\n",
    "    minibatch_pf += total_P_F\n",
    "    minibatch_pb += total_P_B\n",
    "\n",
    "    if (episode + 1) % BATCH_SIZE == 0:\n",
    "      if WANDB:\n",
    "        wandb.log({\n",
    "            \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "            \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "            \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "            \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "            \"z\": minibatch_z/BATCH_SIZE\n",
    "          })\n",
    "        # for name, param in sampler.named_parameters():\n",
    "        #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "      \n",
    "      minibatch_loss.backward(retain_graph=True)\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      minibatch_loss = 0\n",
    "      minibatch_reward = 0\n",
    "      minibatch_z = 0 \n",
    "      minibatch_pf = 0 \n",
    "      minibatch_pb = 0 \n",
    "\n",
    "    pbar.update(1)\n",
    "    episode = episode + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gflowfigure8, f'{run_name}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
