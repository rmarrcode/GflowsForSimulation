{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider, fixed\n",
    "\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "from sigma_graph.data.file_manager import check_dir, find_file_in_dir, load_graph_files\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_EPOCHS = 100000\n",
    "# default = 34\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 10:\n",
    "        return 2\n",
    "    if state == 2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn-complex-coordinate-2024-04-30 21:10:41\n",
      "---------------\n",
      "path_data ./GflowsForSimulation/sigma_graph/data/parsed/\n",
      "/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation\n",
      "data_pat /home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/data/parsed/info_list_pat_0_27.pickle\n"
     ]
    }
   ],
   "source": [
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "config = {\n",
    "    \"custom_model_config\": {\n",
    "        \"custom_model\": \"fcn\", #fcn #attn_fcn\n",
    "        \"reward\": \"complex\", #random_region random single complex\n",
    "        \"reward_interval\": \"step\", #trajectory \n",
    "        \"trajectory_per_reward\": 1,\n",
    "        \"embedding\": \"coordinate\", #number #coordinate\n",
    "        \"is_dynamic_embedding\": False,\n",
    "        \"trajectory_length\": 34,\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"start_node\": 22,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "current_time = datetime.now()\n",
    "run_name = f\"{config['custom_model_config']['custom_model']}-{config['custom_model_config']['reward']}-{config['custom_model_config']['embedding']}-{current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "print(run_name)\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "                \"model_config\": config,\n",
    "                \"exp_config\": {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epocs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE\n",
    "            }\n",
    "        },\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 91/100000 [24:25<446:51:48, 16.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m action_path \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trajectory_length):\n\u001b[0;32m---> 44\u001b[0m   step \u001b[38;5;241m=\u001b[39m \u001b[43mgflowfigure8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEMP_AGENT_INDEX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_node\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     45\u001b[0m   total_P_F \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     46\u001b[0m   total_P_B \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/envs/figure8/gflow_figure8_squad.py:210\u001b[0m, in \u001b[0;36mGlowFigure8Squad.step\u001b[0;34m(self, a_id, reward_nodes)\u001b[0m\n\u001b[1;32m    207\u001b[0m flow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mflow(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[a_id],], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8), device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# update log\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_step_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m action_penalty_red \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_action_red(action)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_action_blue()\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/envs/figure8/gflow_figure8_squad.py:724\u001b[0m, in \u001b[0;36mGlowFigure8Squad._log_step_update\u001b[0;34m(self, prev_obs, actions, rewards)\u001b[0m\n\u001b[1;32m    720\u001b[0m _r \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_agent\u001b[38;5;241m.\u001b[39mget_id()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _agent\u001b[38;5;241m.\u001b[39mget_pos_dir(), _agent\u001b[38;5;241m.\u001b[39mget_encoding(), _agent\u001b[38;5;241m.\u001b[39mget_health()]\n\u001b[1;32m    721\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m _agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteam_red]\n\u001b[1;32m    722\u001b[0m _b \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_agent\u001b[38;5;241m.\u001b[39mget_id()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, _agent\u001b[38;5;241m.\u001b[39mget_pos_dir(), _agent\u001b[38;5;241m.\u001b[39mget_encoding(), _agent\u001b[38;5;241m.\u001b[39mget_health()]\n\u001b[1;32m    723\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m _agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteam_blue]\n\u001b[0;32m--> 724\u001b[0m \u001b[43msave_log_2_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_counter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone_counter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_r\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprev_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/data/file_manager.py:280\u001b[0m, in \u001b[0;36msave_log_2_file\u001b[0;34m(config, n_step, n_done, agents, prev_obs, actions, obs, rewards, dones)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_dir(_log_path):\n\u001b[1;32m    279\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(_log_path)\n\u001b[0;32m--> 280\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_log_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43mdone_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_prefix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# sys.stdout = f\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     _buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep #\u001b[39m\u001b[38;5;132;01m{:2d}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_step)\n",
      "File \u001b[0;32m/usr/lib/python3.8/posixpath.py:77\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mIf any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mwill be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mends with a separator.\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m a \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(a)\n\u001b[0;32m---> 77\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[43m_get_sep\u001b[49m(a)\n\u001b[1;32m     78\u001b[0m path \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "pbar = tqdm(total=NUM_EPOCHS)\n",
    "episode = 0\n",
    "\n",
    "while episode <= NUM_EPOCHS:\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  gflowfigure8._reset_agents()\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "\n",
    "  region_nodes = {15, 11, 10}\n",
    "\n",
    "  if config['custom_model_config']['reward'] == 'random':\n",
    "    reward_node = [random.randint(1, 27)]\n",
    "  elif config['custom_model_config']['reward'] == 'random_region':\n",
    "    reward_node = random.sample(region_nodes, 1)\n",
    "  elif config['custom_model_config']['reward'] == 'single':\n",
    "    reward_node = [17]\n",
    "  elif config['custom_model_config']['reward'] == 'complex':\n",
    "    reward_node = []\n",
    "  \n",
    "  if config['custom_model_config']['is_dynamic_embedding']:  \n",
    "    gflowfigure8.update_reward(reward_node)\n",
    "    \n",
    "  gflowfigure8.reset()\n",
    "\n",
    "  no_trajectory = config['custom_model_config']['trajectory_per_reward']\n",
    "  trajectory_length = config['custom_model_config']['trajectory_length']\n",
    "\n",
    "  for trajectory in range(no_trajectory):\n",
    "    trajectory_path = []\n",
    "    action_path = []\n",
    "    for t in range(trajectory_length):\n",
    "      step = gflowfigure8.step(TEMP_AGENT_INDEX, reward_node)  \n",
    "      total_P_F += step['forward_prob']\n",
    "      total_P_B += step['backward_prob']\n",
    "      total_reward += step['step_reward']\n",
    "      trajectory_path.append(step['red_node'])\n",
    "      action_path.append(step['action'])\n",
    "\n",
    "    logZ = gflowfigure8.sampler.logZ\n",
    "    # TODO find more elegant solution to nan issue\n",
    "    if config['custom_model_config']['reward'] == 'complex':\n",
    "      total_reward += gflowfigure8._episode_rewards()[0]\n",
    "      # maybe +100 is not good\n",
    "      clipped_reward = torch.log(torch.tensor(total_reward+100)).clip(-20)\n",
    "\n",
    "    # clipped_reward = torch.log(torch.tensor(end_rewards)).clip(-20)\n",
    "\n",
    "    loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "\n",
    "    minibatch_loss += loss\n",
    "    minibatch_reward += clipped_reward\n",
    "    minibatch_z += logZ\n",
    "    minibatch_pf += total_P_F\n",
    "    minibatch_pb += total_P_B\n",
    "\n",
    "    if (episode + 1) % BATCH_SIZE == 0:\n",
    "      if WANDB:\n",
    "        wandb.log({\n",
    "            \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "            \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "            \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "            \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "            \"z\": minibatch_z/BATCH_SIZE\n",
    "          })\n",
    "        # for name, param in sampler.named_parameters():\n",
    "        #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "      \n",
    "      minibatch_loss.backward(retain_graph=True)\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      minibatch_loss = 0\n",
    "      minibatch_reward = 0\n",
    "      minibatch_z = 0 \n",
    "      minibatch_pf = 0 \n",
    "      minibatch_pb = 0 \n",
    "\n",
    "    pbar.update(1)\n",
    "    episode = episode + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(gflowfigure8, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(gflowfigure8, f'{run_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
      "[24, 24, 22, 21, 27, 20, 14, 8, 7, 6, 4, 5, 13, 19, 25, 18, 12, 2, 1, 3, 4, 5, 13, 19, 26, 20, 14, 8, 7, 9, 10, 11, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "gflowfigure8.reset()\n",
    "red_path = []\n",
    "blue_path = []\n",
    "for r in range(34):   \n",
    "    step = gflowfigure8.step(0, [])\n",
    "    red_path.append(step['red_node'])\n",
    "    blue_path.append(step['blue_node'])\n",
    "print((red_path))\n",
    "print((blue_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "path_data ./GflowsForSimulation/sigma_graph/data/parsed/\n",
      "/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation\n",
      "data_pat /home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/data/parsed/info_list_pat_0_27.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000d65f99d254a53aa51bc27e2e931fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Graph Index', max=33), Output()), _dom_classes=('widget-â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_graph(index)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_info, _ = load_graph_files(map_lookup=\"S\")\n",
    "col_map = [\"gold\"] * len(map_info.n_info)\n",
    "\n",
    "def display_graph(index):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cur_col_map = col_map[:]\n",
    "    cur_col_map[red_path[index]] = \"red\"\n",
    "    nx.draw_networkx(map_info.g_acs, pos=map_info.n_info, node_color=col_map, edge_color=\"blue\", arrows=True, ax=ax)\n",
    "    ax.set_title(f\"Graph {index}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create an interactive widget to display different graphs\n",
    "slider = IntSlider(min=0, max=33, step=1, value=0, description='Graph Index')\n",
    "interact(display_graph, index=slider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
