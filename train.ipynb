{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 15:32:04,022\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.agents.dqn` has been deprecated. Use `ray.rllib.algorithms.[dqn|simple_q|r2d2|apex_dqn]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "import random\n",
    "\n",
    "INPUT_DIMS = 27\n",
    "OUTPUT_DIMS = 5\n",
    "NUM_EPOCHS = 100000\n",
    "TRAJECTORY_LENGTH = 20\n",
    "BATCH_SIZE = 100\n",
    "START_NODE = 25\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 10:\n",
    "        return 2\n",
    "    if state == 2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "path_data ./GflowsForSimulation/sigma_graph/data/parsed/\n",
      "/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation\n"
     ]
    }
   ],
   "source": [
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epocs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE\n",
    "        }\n",
    "    )\n",
    "\n",
    "config = {\n",
    "    \"custom_model\": \"attn_fcn\", #fcn #attn_fcn\n",
    "    \"reward\": \"complex\", #random_region random 10 complex\n",
    "    \"embedding\": \"coordinate\", #number #coordinate\n",
    "    \"custom_model_config\": {\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "LEARNING_RATE = 3e-4\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "# sampler_fcn -> sampler\n",
    "# TODO: this is bad revisit and come up with something more concise\n",
    "\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%| | 85/100000 [00:28<8:41:44,  3.19i"
     ]
    }
   ],
   "source": [
    "# Using sigma code fully\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "for episode in tqdm.tqdm(range(NUM_EPOCHS), ncols=40):\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  gflowfigure8._reset_agents()\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "\n",
    "  region_nodes = {16, 15, 11, 10, 9}\n",
    "\n",
    "  if config['reward'] == 'random':\n",
    "    reward_node_1i = [random.randint(1, 27)]\n",
    "  elif config['reward'] == 'random_region':\n",
    "    reward_node_1i = random.sample(region_nodes, 1)\n",
    "  elif config['reward'] == '10':\n",
    "    reward_node_1i = [10]\n",
    "  elif config['reward'] == 'complex':\n",
    "    reward_node_1i = []\n",
    "            \n",
    "  for t in range(TRAJECTORY_LENGTH):\n",
    "\n",
    "    step = gflowfigure8.step(TEMP_AGENT_INDEX, reward_node_1i)  \n",
    "    total_P_F += step['forward_prob']\n",
    "    total_P_B += step['backward_prob']\n",
    "    #total_reward += torch.tensor(gflowfigure8._step_reward_test())\n",
    "    total_reward += step['step_reward']\n",
    "\n",
    "  logZ = gflowfigure8.sampler.logZ\n",
    "  # TODO find more elegant solution to nan issue\n",
    "  clipped_reward = torch.log(torch.tensor(total_reward).clip(0)).clip(-20)\n",
    "  loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "\n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += clipped_reward\n",
    "  minibatch_z += logZ\n",
    "  minibatch_pf += total_P_F\n",
    "  minibatch_pb += total_P_B\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    if WANDB:\n",
    "      wandb.log({\n",
    "          \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "          \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "          \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "          \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "          \"z\": minibatch_z/BATCH_SIZE\n",
    "        })\n",
    "      # for name, param in sampler.named_parameters():\n",
    "      #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "    \n",
    "    minibatch_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    minibatch_z = 0 \n",
    "    minibatch_pf = 0 \n",
    "    minibatch_pb = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gflowfigure8, f'{config[\"reward\"]}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: there has to be a better way to do this\n",
    "def norm_dict(unnorm):\n",
    "    min_val = 9999999\n",
    "    for key in unnorm:\n",
    "        min_val = min(unnorm[key], min_val)\n",
    "    for key in unnorm:\n",
    "        unnorm[key] = unnorm[key] + (min(min_val, 0) * -1)\n",
    "    sum = 0\n",
    "    for key in unnorm:\n",
    "        sum += unnorm[key]\n",
    "    for key in unnorm:\n",
    "        unnorm[key] /= sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'reward_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([states[\u001b[38;5;241m0\u001b[39m],], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mgflowfigure8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# (forward_prob, action) = gflowfigure8.probs_to_action(probs)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m total_probs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'reward_nodes'"
     ]
    }
   ],
   "source": [
    "gflowfigure8._reset_agents()\n",
    "state_dirs = {}\n",
    "for node in range(27):\n",
    "    states = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "    states[0][node] = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state = torch.tensor(np.array([states[0],], dtype=np.int8)).to(device)\n",
    "    probs = gflowfigure8.sampler.forward(state, [1])\n",
    "    # (forward_prob, action) = gflowfigure8.probs_to_action(probs)\n",
    "    total_probs = {}\n",
    "    total_probs[\"NOOP\"] = (probs[0]+probs[5]+probs[10]).tolist()\n",
    "    total_probs[\"N\"] = (probs[1]+probs[6]+probs[11]).tolist()\n",
    "    total_probs[\"S\"] = (probs[2]+probs[7]+probs[12]).tolist()\n",
    "    total_probs[\"W\"] = (probs[3]+probs[8]+probs[13]).tolist()\n",
    "    total_probs[\"E\"] = (probs[4]+probs[9]+probs[14]).tolist()\n",
    "    \n",
    "    norm_dict(total_probs)\n",
    "\n",
    "    state_dirs[node] = total_probs\n",
    "\n",
    "json_data = json.dumps(state_dirs, indent=2)\n",
    "with open('logs/temp/flows.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_node_1i [8]\n",
      "25\n",
      "reward_node_1i [19]\n",
      "25\n",
      "reward_node_1i [24]\n",
      "25\n",
      "reward_node_1i [9]\n",
      "25\n",
      "reward_node_1i [11]\n",
      "25\n",
      "reward_node_1i [20]\n",
      "25\n",
      "reward_node_1i [23]\n",
      "19\n",
      "reward_node_1i [21]\n",
      "19\n",
      "reward_node_1i [26]\n",
      "13\n",
      "reward_node_1i [16]\n",
      "13\n",
      "reward_node_1i [20]\n",
      "19\n",
      "reward_node_1i [9]\n",
      "26\n",
      "reward_node_1i [19]\n",
      "19\n",
      "reward_node_1i [16]\n",
      "19\n",
      "reward_node_1i [24]\n",
      "13\n",
      "reward_node_1i [12]\n",
      "19\n",
      "reward_node_1i [5]\n",
      "19\n",
      "reward_node_1i [13]\n",
      "13\n",
      "reward_node_1i [2]\n",
      "13\n",
      "reward_node_1i [1]\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "gflowfigure8 = torch.load('random_reward.pt') \n",
    "trajectory = Trajectory()\n",
    "gflowfigure8._reset_agents()\n",
    "for _ in range(20):   \n",
    "    for a_id in range(config['custom_model_config']['nred']):\n",
    "        step = gflowfigure8.step(a_id)\n",
    "        trajectory.add_step(\n",
    "            forward_prob=step['forward_prob'],\n",
    "            backward_prob=step['backward_prob'],\n",
    "            # flow=step['flow'],\n",
    "            # action=step['action'],\n",
    "            reward=step['step_reward'],\n",
    "            # node=step['node']\n",
    "        )\n",
    "        print(step['node'])\n",
    "        #print(step['action'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
