{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 17:37:48,034\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.agents.dqn` has been deprecated. Use `ray.rllib.algorithms.[dqn|simple_q|r2d2|apex_dqn]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider, fixed\n",
    "\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "from sigma_graph.data.file_manager import check_dir, find_file_in_dir, load_graph_files\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_EPOCHS = 140000\n",
    "# default = 34\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 17:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn-single-coordinate-2024-05-02 17:37:48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr-marr747\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/wandb/run-20240502_173751-56v7j5aj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/56v7j5aj' target=\"_blank\">fcn-single-coordinate-2024-05-02 17:37:48</a></strong> to <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/56v7j5aj' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation/runs/56v7j5aj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_pat /home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/data/parsed/info_list_pat_0_27.pickle\n"
     ]
    }
   ],
   "source": [
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "config = {\n",
    "    \"custom_model_config\": {\n",
    "        \"custom_model\": \"fcn\", #fcn #attn_fcn\n",
    "        \"reward\": \"single\", #random_region random single complex\n",
    "        \"reward_interval\": \"step\", #trajectory \n",
    "        \"trajectory_per_reward\": 1,\n",
    "        \"embedding\": \"coordinate\", #number #coordinate\n",
    "        \"is_dynamic_embedding\": False,\n",
    "        \"trajectory_length\": 5,\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"start_node\": 22,\n",
    "        \"reward_node\": 17,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "current_time = datetime.now()\n",
    "run_name = f\"{config['custom_model_config']['custom_model']}-{config['custom_model_config']['reward']}-{config['custom_model_config']['embedding']}-{current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "print(run_name)\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "                \"model_config\": config,\n",
    "                \"exp_config\": {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epocs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE\n",
    "            }\n",
    "        },\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler_fcn_simple.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1672/10000 [00:14<01:03, 131.90it/s]"
     ]
    }
   ],
   "source": [
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "pbar = tqdm(total=NUM_EPOCHS)\n",
    "episode = 0\n",
    "\n",
    "while episode <= NUM_EPOCHS:\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "\n",
    "  reward_node = config['custom_model_config']['reward_node']\n",
    "  reward_node = [reward_node]\n",
    "\n",
    "  gflowfigure8.reset()\n",
    "\n",
    "  trajectory_length = config['custom_model_config']['trajectory_length']\n",
    "\n",
    "  trajectory_path = []\n",
    "  action_path = []\n",
    "  \n",
    "  for t in range(trajectory_length):\n",
    "    step = gflowfigure8.step_fcn_simple(TEMP_AGENT_INDEX)  \n",
    "    total_P_F += step['forward_prob']\n",
    "    total_P_B += step['backward_prob']\n",
    "    trajectory_path.append(step['red_node'])\n",
    "    action_path.append(step['action'])\n",
    "\n",
    "  logZ = gflowfigure8.sampler_fcn_simple.logZ\n",
    "  \n",
    "  last_red_node = gflowfigure8.team_red[TEMP_AGENT_INDEX].get_info()[\"node\"]\n",
    "  total_reward = compute_reward(last_red_node)\n",
    "  clipped_reward = torch.log(torch.tensor(total_reward)).clip(-20)\n",
    "\n",
    "  loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "\n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += clipped_reward\n",
    "  minibatch_z += logZ\n",
    "  minibatch_pf += total_P_F\n",
    "  minibatch_pb += total_P_B\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    if WANDB:\n",
    "      wandb.log({\n",
    "          \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "          \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "          \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "          \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "          \"z\": minibatch_z/BATCH_SIZE\n",
    "        })\n",
    "      # for name, param in sampler.named_parameters():\n",
    "      #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "    \n",
    "    minibatch_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    minibatch_z = 0 \n",
    "    minibatch_pf = 0 \n",
    "    minibatch_pb = 0 \n",
    "\n",
    "  pbar.update(1)\n",
    "  episode = episode + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]\n"
     ]
    }
   ],
   "source": [
    "for t in range(trajectory_length):\n",
    "  step = gflowfigure8.step_fcn_simple(TEMP_AGENT_INDEX, reward_node)  \n",
    "  total_P_F += step['forward_prob']\n",
    "  total_P_B += step['backward_prob']\n",
    "  trajectory_path.append(step['red_node'])\n",
    "print(trajectory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
