{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn-complex-number-2024-05-19 21:51:04\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cu6p88en) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f871e9f4651b456cb22ba8d3bdcbc1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▁</td></tr><tr><td>pb</td><td>▁</td></tr><tr><td>pf</td><td>▁</td></tr><tr><td>reward</td><td>▁</td></tr><tr><td>z</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>815.55496</td></tr><tr><td>pb</td><td>-16.47918</td></tr><tr><td>pf</td><td>-8.303</td></tr><tr><td>reward</td><td>-8.55148</td></tr><tr><td>z</td><td>10.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fcn-complex-number-2024-05-19 21:50:35</strong> at: <a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/cu6p88en' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation/runs/cu6p88en</a><br/> View project at: <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240519_215036-cu6p88en/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cu6p88en). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad99c60ac2a94f00b29514aa6f202f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113076636360752, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/wandb/run-20240519_215104-zj1kicbp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/zj1kicbp' target=\"_blank\">fcn-complex-number-2024-05-19 21:51:04</a></strong> to <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/zj1kicbp' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation/runs/zj1kicbp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider, fixed\n",
    "\n",
    "from sigma_graph.data.file_manager import load_graph_files, save_log_2_file, log_done_reward\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "from sigma_graph.data.file_manager import check_dir, find_file_in_dir, load_graph_files\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_EPOCHS = 100000\n",
    "# default = 34\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 17:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]\n",
    "\n",
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "config = {\n",
    "    \"custom_model_config\": {\n",
    "        \"custom_model\": \"fcn\", #fcn #attn_fcn\n",
    "        \"reward\": \"complex\", #random_region random single complex\n",
    "        \"reward_interval\": \"step\", #trajectory \n",
    "        \"trajectory_per_reward\": 1,\n",
    "        \"embedding\": \"number\", #number #coordinate\n",
    "        \"is_dynamic_embedding\": False,\n",
    "        \"trajectory_length\": 5,\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"start_node\": 24,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "current_time = datetime.now()\n",
    "run_name = f\"{config['custom_model_config']['custom_model']}-{config['custom_model_config']['reward']}-{config['custom_model_config']['embedding']}-{current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "print(run_name)\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "                \"model_config\": config,\n",
    "                \"exp_config\": {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epocs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE\n",
    "            }\n",
    "        },\n",
    "        name=run_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT  coordinate time\n",
    "\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler_fig8_gat_coordinate_time.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "pbar = tqdm(total=NUM_EPOCHS)\n",
    "episode = 0\n",
    "\n",
    "while episode <= NUM_EPOCHS:\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "    \n",
    "  trajectory_length = config['custom_model_config']['trajectory_length']\n",
    "\n",
    "  trajectory_path = []\n",
    "  action_path = []\n",
    "  gflowfigure8.reset()\n",
    "  gflowfigure8.reset_state_gat_coordinate_time()\n",
    "  for t in range(trajectory_length):\n",
    "    step = gflowfigure8.step_gat_coordinate_time(TEMP_AGENT_INDEX)  \n",
    "    total_P_F += step['forward_prob']\n",
    "    total_P_B += step['backward_prob']\n",
    "    total_reward += step['step_reward']\n",
    "    trajectory_path.append(step['red_node'])\n",
    "    action_path.append(step['action'])\n",
    "\n",
    "  logZ = gflowfigure8.sampler_fcn_coordinate_time.logZ\n",
    "    \n",
    "  clipped_reward = torch.log(torch.tensor(total_reward)).clip(-20)\n",
    "  #last_node = gflowfigure8.team_red[0].get_info()[\"node\"]\n",
    "  #clipped_reward = torch.log(torch.tensor(compute_reward(last_node))).clip(-20)\n",
    "\n",
    "  loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "  \n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += clipped_reward\n",
    "  minibatch_z += logZ\n",
    "  minibatch_pf += total_P_F\n",
    "  minibatch_pb += total_P_B\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    if WANDB:\n",
    "      wandb.log({\n",
    "          \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "          \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "          \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "          \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "          \"z\": minibatch_z/BATCH_SIZE\n",
    "        })\n",
    "      # for name, param in sampler.named_parameters():\n",
    "      #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "    \n",
    "    minibatch_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    minibatch_z = 0 \n",
    "    minibatch_pf = 0 \n",
    "    minibatch_pb = 0 \n",
    "\n",
    "  pbar.update(1)\n",
    "  episode = episode + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gflowfigure8, f'models/fcn-complex-number-2024-05-16 19:49:08.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_length = config[\"custom_model_config\"][\"trajectory_length\"]\n",
    "gflowfigure8.reset()\n",
    "gflowfigure8.reset_state_gat_coordinate_time()\n",
    "red_path = []\n",
    "blue_path = []\n",
    "step_rewards = []\n",
    "total_reward = 0\n",
    "for r in range(trajectory_length):   \n",
    "    step = gflowfigure8.step_gat_coordinate_time(0)\n",
    "    red_path.append(step['red_node'])\n",
    "    blue_path.append(step['blue_node'])\n",
    "    step_rewards.append(step['step_reward'])\n",
    "    total_reward += step['step_reward']\n",
    "\n",
    "map_info, _ = load_graph_files(map_lookup=\"S\")\n",
    "col_map = [\"gold\"] * len(map_info.n_info)\n",
    "\n",
    "def display_graph(index):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cur_col_map = col_map[:]\n",
    "    cur_col_map[red_path[index]-1] = \"red\"\n",
    "    cur_col_map[blue_path[index]-1] = \"blue\"\n",
    "    nx.draw_networkx(map_info.g_acs, pos=map_info.n_info, node_color=cur_col_map, edge_color=\"blue\", arrows=True, ax=ax)\n",
    "    ax.set_title(f\"step rewards {step_rewards[index]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create an interactive widget to display different graphs\n",
    "slider = IntSlider(min=0, max=trajectory_length-1, step=1, value=0, description='Graph Index')\n",
    "interact(display_graph, index=slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/.venv/lib/python3.8/site-packages/wandb/sdk/wandb_run.py:2265: UserWarning: Run (cu6p88en) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "  0%|          | 165/100000 [00:46<7:47:18,  3.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m\n\u001b[1;32m     52\u001b[0m   wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     53\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: minibatch_loss\u001b[38;5;241m/\u001b[39mBATCH_SIZE, \n\u001b[1;32m     54\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m:  minibatch_reward\u001b[38;5;241m/\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m: minibatch_z\u001b[38;5;241m/\u001b[39mBATCH_SIZE\n\u001b[1;32m     58\u001b[0m     })\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# for name, param in sampler.named_parameters():\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m#     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mminibatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/.venv/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FCN coordinate time\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler_fcn_coordinate_time.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "pbar = tqdm(total=NUM_EPOCHS)\n",
    "episode = 0\n",
    "\n",
    "while episode <= NUM_EPOCHS:\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "    \n",
    "  trajectory_length = config['custom_model_config']['trajectory_length']\n",
    "\n",
    "  trajectory_path = []\n",
    "  action_path = []\n",
    "  gflowfigure8.reset()\n",
    "  gflowfigure8.reset_state_fcn_coordinate_time()\n",
    "  for t in range(trajectory_length):\n",
    "    step = gflowfigure8.step_fcn_coordinate_time(TEMP_AGENT_INDEX)  \n",
    "    total_P_F += step['forward_prob']\n",
    "    total_P_B += step['backward_prob']\n",
    "    total_reward += step['step_reward']\n",
    "    trajectory_path.append(step['red_node'])\n",
    "    action_path.append(step['action'])\n",
    "\n",
    "  logZ = gflowfigure8.sampler_fcn_coordinate_time.logZ\n",
    "    \n",
    "  clipped_reward = torch.log(torch.tensor(total_reward)).clip(-20)\n",
    "  #last_node = gflowfigure8.team_red[0].get_info()[\"node\"]\n",
    "  #clipped_reward = torch.log(torch.tensor(compute_reward(last_node))).clip(-20)\n",
    "\n",
    "  loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "  \n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += clipped_reward\n",
    "  minibatch_z += logZ\n",
    "  minibatch_pf += total_P_F\n",
    "  minibatch_pb += total_P_B\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    if WANDB:\n",
    "      wandb.log({\n",
    "          \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "          \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "          \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "          \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "          \"z\": minibatch_z/BATCH_SIZE\n",
    "        })\n",
    "      # for name, param in sampler.named_parameters():\n",
    "      #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "    \n",
    "    minibatch_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    minibatch_z = 0 \n",
    "    minibatch_pf = 0 \n",
    "    minibatch_pb = 0 \n",
    "\n",
    "  pbar.update(1)\n",
    "  episode = episode + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "gflowfigure8.reset_state_fcn_coordinate_time()\n",
    "trajectory_length = config[\"custom_model_config\"][\"trajectory_length\"]\n",
    "red_path = []\n",
    "blue_path = []\n",
    "step_rewards = []\n",
    "total_reward = 0\n",
    "for r in range(trajectory_length):   \n",
    "    step = gflowfigure8.step_fcn_coordinate_time(0)\n",
    "    red_path.append(step['red_node'])\n",
    "    blue_path.append(step['blue_node'])\n",
    "    step_rewards.append(step['step_reward'])\n",
    "    total_reward += step['step_reward']\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# deterministic trajectory sample\n",
    "gflowfigure8.reset_state_fcn_coordinate_time()\n",
    "trajectory_length = config[\"custom_model_config\"][\"trajectory_length\"]\n",
    "red_path = []\n",
    "blue_path = []\n",
    "step_rewards = []\n",
    "total_reward = 0\n",
    "for r in range(trajectory_length):  \n",
    "    step = gflowfigure8.step_fcn_coordinate_time_deterministic(0)\n",
    "    red_path.append(step['red_node'])\n",
    "    blue_path.append(step['blue_node'])\n",
    "    step_rewards.append(step['step_reward'])\n",
    "    total_reward += step['step_reward']\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e009b4f884857bf8e71002019c880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Graph Index', max=4), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_graph(index)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(total_reward)\n",
    "\n",
    "map_info, _ = load_graph_files(map_lookup=\"S\")\n",
    "col_map = [\"gold\"] * len(map_info.n_info)\n",
    "\n",
    "def display_graph(index):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cur_col_map = col_map[:]\n",
    "    cur_col_map[red_path[index]-1] = \"red\"\n",
    "    cur_col_map[blue_path[index]-1] = \"blue\"\n",
    "    nx.draw_networkx(map_info.g_acs, pos=map_info.n_info, node_color=cur_col_map, edge_color=\"blue\", arrows=True, ax=ax)\n",
    "    ax.set_title(f\"step rewards {step_rewards[index]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create an interactive widget to display different graphs\n",
    "slider = IntSlider(min=0, max=trajectory_length-1, step=1, value=0, description='Graph Index')\n",
    "interact(display_graph, index=slider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
