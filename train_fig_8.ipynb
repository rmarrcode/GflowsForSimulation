{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 15:51:33,447\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.agents.dqn` has been deprecated. Use `ray.rllib.algorithms.[dqn|simple_q|r2d2|apex_dqn]` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider, fixed\n",
    "\n",
    "from sigma_graph.envs.figure8.action_lookup import MOVE_LOOKUP, TURN_90_LOOKUP\n",
    "from sigma_graph.envs.figure8.default_setup import OBS_TOKEN\n",
    "from sigma_graph.envs.figure8.figure8_squad_rllib import Figure8SquadRLLib\n",
    "from sigma_graph.envs.figure8.gflow_figure8_squad import GlowFigure8Squad\n",
    "#from graph_scout.envs.base import ScoutMissionStdRLLib\n",
    "import sigma_graph.envs.figure8.default_setup as default_setup\n",
    "from sigma_graph.data.file_manager import check_dir, find_file_in_dir, load_graph_files\n",
    "import model  # THIS NEEDS TO BE HERE IN ORDER TO RUN __init__.py!\n",
    "import model.utils as utils\n",
    "import model.gnn_gflow \n",
    "from trajectory import Trajectory\n",
    "import losses\n",
    "\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUM_EPOCHS = 100000\n",
    "# default = 34\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WANDB = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_action_move = {\n",
    "    0: \"NOOP\",\n",
    "    1: \"N\",\n",
    "    2: \"S\",\n",
    "    3: \"W\",\n",
    "    4: \"E\",\n",
    "}\n",
    "\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def compute_reward(state):\n",
    "    if state == 17:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def convert_discrete_action_to_multidiscrete(action):\n",
    "        return [action % len(local_action_move), action // len(local_action_move)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn-complex-number-2024-05-04 15:51:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mr-marr747\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rmarr/Documents/GflowsForSimulation_env/GflowsForSimulation/wandb/run-20240504_155134-idn01kbc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/idn01kbc' target=\"_blank\">fcn-complex-number-2024-05-04 15:51:33</a></strong> to <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/r-marr747/graph-training-simulation' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/r-marr747/graph-training-simulation/runs/idn01kbc' target=\"_blank\">https://wandb.ai/r-marr747/graph-training-simulation/runs/idn01kbc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Investigate loss rewar mirror\n",
    "# Try real reward\n",
    "# Make code cleaner \n",
    "# visualize flows \n",
    "\n",
    "config = {\n",
    "    \"custom_model_config\": {\n",
    "        \"custom_model\": \"fcn\", #fcn #attn_fcn\n",
    "        \"reward\": \"complex\", #random_region random single complex\n",
    "        \"reward_interval\": \"step\", #trajectory \n",
    "        \"trajectory_per_reward\": 1,\n",
    "        \"embedding\": \"number\", #number #coordinate\n",
    "        \"is_dynamic_embedding\": False,\n",
    "        \"trajectory_length\": 4,\n",
    "        \"nred\": 1,\n",
    "        \"nblue\": 1,\n",
    "        \"start_node\": 22,\n",
    "        \"aggregation_fn\": \"agent_node\",\n",
    "        \"hidden_size\": 15,\n",
    "        \"is_hybrid\": False,\n",
    "        \"conv_type\": \"gcn\",\n",
    "        \"layernorm\": False,\n",
    "        \"graph_obs_token\": {\"embed_opt\": False, \"embed_dir\": True},\n",
    "    },\n",
    "    \"env_config\": {\n",
    "        \"env_path\": \".\",\n",
    "        \"act_masked\": True,\n",
    "        \"init_red\": None,\n",
    "        \"init_blue\": None,\n",
    "        \"init_health_red\": 20,\n",
    "        \"init_health_blue\": 20,\n",
    "        \"obs_embed\": False,\n",
    "        \"obs_dir\": False,\n",
    "        \"obs_team\": True,\n",
    "        \"obs_sight\": False,\n",
    "        \"log_on\": True,\n",
    "        \"log_path\": \"logs/temp/\",\n",
    "        \"fixed_start\": -1,\n",
    "        \"penalty_stay\": 0,\n",
    "        \"threshold_damage_2_blue\": 2,\n",
    "        \"threshold_damage_2_red\": 5,\n",
    "    },\n",
    "}\n",
    "\n",
    "current_time = datetime.now()\n",
    "run_name = f\"{config['custom_model_config']['custom_model']}-{config['custom_model_config']['reward']}-{config['custom_model_config']['embedding']}-{current_time.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "print(run_name)\n",
    "\n",
    "if WANDB:\n",
    "    wandb.init(\n",
    "        project=\"graph-training-simulation\",\n",
    "        config={\n",
    "                \"model_config\": config,\n",
    "                \"exp_config\": {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epocs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE\n",
    "            }\n",
    "        },\n",
    "        name=run_name\n",
    "    )\n",
    "\n",
    "gflowfigure8 = GlowFigure8Squad(sampler_config=config)\n",
    "optimizer = optim.AdamW(gflowfigure8.sampler_fig8.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35032/100000 [07:27<22:14, 48.70it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m action_path \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trajectory_length):\n\u001b[0;32m---> 29\u001b[0m   step \u001b[38;5;241m=\u001b[39m \u001b[43mgflowfigure8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_fcn_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTEMP_AGENT_INDEX\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     30\u001b[0m   total_P_F \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m   total_P_B \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/sigma_graph/envs/figure8/gflow_figure8_squad.py:257\u001b[0m, in \u001b[0;36mGlowFigure8Squad.step_fcn_complex\u001b[0;34m(self, a_id)\u001b[0m\n\u001b[1;32m    254\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[a_id],], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint8), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m#probs_forward = self.sampler_fig8.forward(obs, self.step_counter)\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m probs_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler_fig8\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# TODO fix this\u001b[39;00m\n\u001b[1;32m    260\u001b[0m cat \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mprobs_forward)\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/model/samplers.py:564\u001b[0m, in \u001b[0;36mSamplerFig8CoordinateTime.forward\u001b[0;34m(self, obs, reward_nodes)\u001b[0m\n\u001b[1;32m    561\u001b[0m bool_obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mbool()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    562\u001b[0m cur_node \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_loc(bool_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 564\u001b[0m reward_nodes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m (node \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m reward_nodes \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m0.0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_size)])\n\u001b[1;32m    565\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_embedding, reward_nodes), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    566\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_forward(final_embeddings[cur_node\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GflowsForSimulation_env/GflowsForSimulation/model/samplers.py:564\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    561\u001b[0m bool_obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mbool()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    562\u001b[0m cur_node \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_loc(bool_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 564\u001b[0m reward_nodes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreward_nodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_size)])\n\u001b[1;32m    565\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_embedding, reward_nodes), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    566\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_forward(final_embeddings[cur_node\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35034/100000 [07:40<22:14, 48.70it/s]"
     ]
    }
   ],
   "source": [
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "minibatch_z = 0\n",
    "minibatch_pf = 0\n",
    "minibatch_pb = 0\n",
    "\n",
    "pbar = tqdm(total=NUM_EPOCHS)\n",
    "episode = 0\n",
    "\n",
    "#remove\n",
    "gflowfigure8.update_reward([17])\n",
    "#remove\n",
    "\n",
    "while episode <= NUM_EPOCHS:\n",
    "  \n",
    "  TEMP_AGENT_INDEX = 0\n",
    "  \n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  total_reward = 0\n",
    "    \n",
    "  gflowfigure8.reset()\n",
    "\n",
    "  trajectory_length = config['custom_model_config']['trajectory_length']\n",
    "\n",
    "  trajectory_path = []\n",
    "  action_path = []\n",
    "  for t in range(trajectory_length):\n",
    "    step = gflowfigure8.step_fcn_complex(TEMP_AGENT_INDEX)  \n",
    "    total_P_F += step['forward_prob']\n",
    "    total_P_B += step['backward_prob']\n",
    "    total_reward += step['step_reward']\n",
    "    trajectory_path.append(step['red_node'])\n",
    "    action_path.append(step['action'])\n",
    "\n",
    "  logZ = gflowfigure8.sampler_fig8.logZ\n",
    "    \n",
    "  #clipped_reward = torch.log(torch.tensor(total_reward)).clip(-20)\n",
    "  last_node = gflowfigure8.team_red[0].get_info()[\"node\"]\n",
    "  clipped_reward = torch.log(torch.tensor(compute_reward(last_node))).clip(-20)\n",
    "\n",
    "  loss = (logZ + total_P_F - clipped_reward - total_P_B).pow(2)\n",
    "\n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += clipped_reward\n",
    "  minibatch_z += logZ\n",
    "  minibatch_pf += total_P_F\n",
    "  minibatch_pb += total_P_B\n",
    "\n",
    "  if (episode + 1) % BATCH_SIZE == 0:\n",
    "    if WANDB:\n",
    "      wandb.log({\n",
    "          \"loss\": minibatch_loss/BATCH_SIZE, \n",
    "          \"reward\":  minibatch_reward/BATCH_SIZE,\n",
    "          \"pf\": minibatch_pf/BATCH_SIZE,\n",
    "          \"pb\": minibatch_pb/BATCH_SIZE,\n",
    "          \"z\": minibatch_z/BATCH_SIZE\n",
    "        })\n",
    "      # for name, param in sampler.named_parameters():\n",
    "      #     wandb.log({f\"{name}_mean\": param.data.mean().item(), f\"{name}_std\": param.data.std().item()})\n",
    "    \n",
    "    minibatch_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    minibatch_z = 0 \n",
    "    minibatch_pf = 0 \n",
    "    minibatch_pb = 0 \n",
    "\n",
    "  pbar.update(1)\n",
    "  episode = episode + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gflowfigure8, f'models/{run_name}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
