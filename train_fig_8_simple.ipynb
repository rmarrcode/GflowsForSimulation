{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "import tqdm\n",
    "import matplotlib.pyplot as pp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMS = 27\n",
    "OUTPUT_DIMS = 5\n",
    "NUM_EPOCHS = 50000\n",
    "TRAJECTORY_LENGTH = 20\n",
    "BATCH_SIZE = 10\n",
    "START_NODE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBModel(nn.Module):\n",
    "  def __init__(self, num_hid):\n",
    "    super().__init__()\n",
    "    # The input dimension is 6 for the 6 patches.\n",
    "    self.mlp = nn.Sequential(nn.Linear(INPUT_DIMS, num_hid), nn.LeakyReLU(),\n",
    "                             # We now output 12 numbers, 6 for P_F and 6 for P_B\n",
    "                             nn.Linear(num_hid, OUTPUT_DIMS*2))\n",
    "    # log Z is just a single number\n",
    "    self.logZ = nn.Parameter(torch.ones(1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.mlp(x)\n",
    "    # Slice the logits, and mask invalid actions (since we're predicting\n",
    "    # log-values), we use -100 since exp(-100) is tiny, but we don't want -inf)\n",
    "    P_F = logits[..., :OUTPUT_DIMS] #* (1 - x) + x * -100\n",
    "    P_B = logits[..., OUTPUT_DIMS:] #* x + (1 - x) * -100\n",
    "    return P_F, P_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = {\n",
    "    1: {0: 1, 1: 2, 2: 1, 3: 3, 4: 1},\n",
    "    2: {0: 2, 1: 12, 2: 1, 3: 2, 4: 2},\n",
    "    3: {0: 3, 1: 3, 2: 3, 3: 4, 4: 1},\n",
    "    4: {0: 4, 1: 5, 2: 4, 3: 6, 4: 3},\n",
    "    5: {0: 5, 1: 13, 2: 4, 3: 5, 4: 5},\n",
    "    6: {0: 6, 1: 6, 2: 6, 3: 7, 4: 4},\n",
    "    7: {0: 7, 1: 8, 2: 7, 3: 9, 4: 6},\n",
    "    8: {0: 8, 1: 14, 2: 7, 3: 8, 4: 8},\n",
    "    9: {0: 9, 1: 9, 2: 9, 3: 10, 4: 7},\n",
    "    10: {0: 10, 1: 11, 2: 10, 3: 10, 4: 9},\n",
    "    11: {0: 11, 1: 15, 2: 10, 3: 16, 4: 11},\n",
    "    12: {0: 12, 1: 18, 2: 2, 3: 12, 4: 12},\n",
    "    13: {0: 13, 1: 19, 2: 5, 3: 13, 4: 13},\n",
    "    14: {0: 14, 1: 20, 2: 8, 3: 14, 4: 14},\n",
    "    15: {0: 15, 1: 21, 2: 11, 3: 17, 4: 15},\n",
    "    16: {0: 16, 1: 17, 2: 16, 3: 16, 4: 11},\n",
    "    17: {0: 17, 1: 22, 2: 16, 3: 23, 4: 15},\n",
    "    18: {0: 18, 1: 18, 2: 12, 3: 25, 4: 18},\n",
    "    19: {0: 19, 1: 19, 2: 13, 3: 26, 4: 25},\n",
    "    20: {0: 20, 1: 20, 2: 14, 3: 27, 4: 26},\n",
    "    21: {0: 21, 1: 21, 2: 15, 3: 22, 4: 27},\n",
    "    22: {0: 22, 1: 22, 2: 17, 3: 24, 4: 21},\n",
    "    23: {0: 23, 1: 24, 2: 17, 3: 23, 4: 17},\n",
    "    24: {0: 24, 1: 22, 2: 23, 3: 24, 4: 22},\n",
    "    25: {0: 25, 1: 25, 2: 25, 3: 19, 4: 18},\n",
    "    26: {0: 26, 1: 26, 2: 26, 3: 20, 4: 19},\n",
    "    27: {0: 27, 1: 27, 2: 27, 3: 21, 4: 20}\n",
    "}\n",
    "def state_to_vec(state):\n",
    "    result = [0]*27\n",
    "    result[state-1] = 1\n",
    "    return torch.tensor(result).float()\n",
    "\n",
    "def reward(state):\n",
    "    if state == 10:\n",
    "        return 2\n",
    "    if state == 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|         | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_state 18\n",
      "new_state 18\n",
      "new_state 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m total_P_F \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m cat\u001b[39m.\u001b[39mlog_prob(action)\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m t \u001b[39m==\u001b[39m TRAJECTORY_LENGTH\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     34\u001b[0m   \u001b[39m# If we've built a complete face, we're done, so the reward is > 0\u001b[39;00m\n\u001b[1;32m     35\u001b[0m   \u001b[39m# (unless the face is invalid)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m   reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward(new_state)])\n\u001b[1;32m     37\u001b[0m \u001b[39m# We recompute P_F and P_B for new_state\u001b[39;00m\n\u001b[1;32m     38\u001b[0m P_F_s, P_B_s \u001b[39m=\u001b[39m model(state_to_vec(new_state))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Instantiate model and optimizer\n",
    "model = TBModel(512)\n",
    "opt = torch.optim.Adam(model.parameters(),  3e-4)\n",
    "\n",
    "# Let's keep track of the losses and the faces we sample\n",
    "tb_losses = []\n",
    "tb_sampled_faces = []\n",
    "tb_rewards = []\n",
    "logZs = []\n",
    "\n",
    "# To not complicate the code, I'll just accumulate losses here and take a\n",
    "# gradient step every `update_freq` episode.\n",
    "minibatch_loss = 0\n",
    "minibatch_reward = 0\n",
    "\n",
    "for episode in tqdm.tqdm(range(NUM_EPOCHS), ncols=40):\n",
    "  # Each episode starts with an \"empty state\"\n",
    "  state = START_NODE\n",
    "  # Predict P_F, P_B\n",
    "  P_F_s, P_B_s = model(state_to_vec(state))\n",
    "  total_P_F = 0\n",
    "  total_P_B = 0\n",
    "  for t in range(TRAJECTORY_LENGTH):\n",
    "    # Here P_F is logits, so we want the Categorical to compute the softmax for us\n",
    "    cat = Categorical(logits=P_F_s)\n",
    "    action = cat.sample()\n",
    "    # \"Go\" to the next state\n",
    "    new_state = transitions[state][action.item()]\n",
    "    # Accumulate the P_F sum\n",
    "    total_P_F += cat.log_prob(action)\n",
    "\n",
    "    if t == TRAJECTORY_LENGTH-1:\n",
    "      # If we've built a complete face, we're done, so the reward is > 0\n",
    "      # (unless the face is invalid)\n",
    "      reward = torch.tensor(reward(new_state))\n",
    "    # We recompute P_F and P_B for new_state\n",
    "    P_F_s, P_B_s = model(state_to_vec(new_state))\n",
    "    # Here we accumulate P_B, going backwards from `new_state`. We're also just\n",
    "    # going to use opposite semantics for the backward policy. I.e., for P_F action\n",
    "    # `i` just added the face part `i`, for P_B we'll assume action `i` removes\n",
    "    # face part `i`, this way we can just keep the same indices.\n",
    "    total_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
    "\n",
    "    # Continue iterating\n",
    "    state = new_state\n",
    "\n",
    "  # We're done with the trajectory, let's compute its loss. Since the reward can\n",
    "  # sometimes be zero, instead of log(0) we'll clip the log-reward to -20.\n",
    "  loss = (model.logZ + total_P_F - torch.log(reward).clip(-20) - total_P_B).pow(2)\n",
    "  minibatch_loss += loss\n",
    "  minibatch_reward += reward\n",
    "\n",
    "\n",
    "  # Add the face to the list, and if we are at an\n",
    "  # update episode, take a gradient step.\n",
    "  tb_sampled_faces.append(state)\n",
    "  if episode % BATCH_SIZE == 0:\n",
    "    tb_losses.append(minibatch_loss.item())\n",
    "    tb_rewards.append(minibatch_reward.item())\n",
    "    minibatch_loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    minibatch_loss = 0\n",
    "    minibatch_reward = 0\n",
    "    logZs.append(model.logZ.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = pp.subplots(3, 1, figsize=(10,6))\n",
    "pp.sca(ax[0])\n",
    "pp.plot(tb_losses)\n",
    "pp.yscale('log')\n",
    "pp.ylabel('loss')\n",
    "pp.sca(ax[1])\n",
    "pp.plot(np.exp(logZs))\n",
    "pp.ylabel('estimated Z')\n",
    "pp.sca(ax[2])\n",
    "pp.plot(tb_rewards)\n",
    "pp.ylabel('rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cogn_arch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
